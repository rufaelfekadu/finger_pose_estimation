{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from threading import Thread\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from util.data import *\n",
    "from config import cfg\n",
    "from data import BaseDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = self.generate_encoding(d_model, max_len)\n",
    "\n",
    "    def generate_encoding(self, d_model, max_len):\n",
    "        encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        encoding = encoding.unsqueeze(0)\n",
    "        return encoding\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_length = x.size(1)\n",
    "        return self.encoding[:, :seq_length].to(x.device)\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, num_heads):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        self.pos_encoder = PositionalEncoding(hidden_dim)\n",
    "        self.encoder_layers = nn.TransformerEncoderLayer(hidden_dim, num_heads)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layers, num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x + self.pos_encoder(x)\n",
    "        x = x.permute(1, 0, 2)  # (seq_len, batch_size, hidden_dim)\n",
    "        output = self.encoder(x)\n",
    "        return output\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dim, num_layers, num_heads):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.embedding = nn.Linear(output_dim, hidden_dim)\n",
    "        self.pos_decoder = PositionalEncoding(hidden_dim)\n",
    "        self.decoder_layers = nn.TransformerDecoderLayer(hidden_dim, num_heads)\n",
    "        self.decoder = nn.TransformerDecoder(self.decoder_layers, num_layers)\n",
    "\n",
    "    def generate_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, x, encoder_output):\n",
    "        x = self.embedding(x)\n",
    "        x = x + self.pos_decoder(x)\n",
    "        x = x.permute(1, 0, 2)  # (seq_len, batch_size, hidden_dim)\n",
    "\n",
    "        # Generate causal mask\n",
    "        tgt_mask = self.generate_mask(x.size(0)).to(x.device)\n",
    "\n",
    "        output = self.decoder(x, encoder_output, tgt_mask=tgt_mask)\n",
    "        return output\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, num_layers, num_heads):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = TransformerEncoder(input_dim, hidden_dim, num_layers, num_heads)\n",
    "        self.decoder = TransformerDecoder(output_dim, hidden_dim, num_layers, num_heads)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.pos_encoder = PositionalEncoding(hidden_dim)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        encoder_output = self.encoder(x)\n",
    "        decoder_output = self.decoder(y, encoder_output)\n",
    "        output = self.fc(decoder_output)\n",
    "        # reshape back to batch_size x seq_len x num_channels\n",
    "        output = output.permute(1, 0, 2)\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "    \n",
    "# Encoder layer\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, scale_factor=(2,2)):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=(3,2), stride=(1,1), padding=(2,1)),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(3,3), stride=scale_factor),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x.float())\n",
    "        return x\n",
    "    \n",
    "class Conv3DEncoderLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, scale_factor=(2,2)):\n",
    "        super(Conv3DEncoderLayer, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, kernel_size=(3,3,2), stride=(1,1,1), padding=(1,1,1)),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(3,3,3), stride=scale_factor),\n",
    "        )\n",
    "    \n",
    "# Decoder layer\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, last=False, scale_factor=(2,2), output_shape=(1000,20)):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=(3,2), stride=(1,1)),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        if last:\n",
    "            self.decoder.append(nn.Upsample(size=output_shape, mode='bilinear', align_corners=False))\n",
    "        else:\n",
    "            self.decoder.append(nn.Upsample(scale_factor=scale_factor, mode='bilinear', align_corners=False))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "# Construct a model with 3 conv layers 3 residual blocks and 3 deconv layers using the ResNet architecture\n",
    "class NeuroPose(nn.Module):\n",
    "    def __init__(self, in_channels=1, num_residual_blocks=3, output_shape=(1000,20)):\n",
    "        super(NeuroPose, self).__init__()\n",
    "        \n",
    "        encoder_channels = [in_channels, 32, 128, 256]\n",
    "        scale_factors = [(5,2), (4,2), (2,2)]\n",
    "\n",
    "        self.encoder = self.make_encoder_layers(channels=encoder_channels, scale_factors=scale_factors)\n",
    "\n",
    "        # get last number of filters from encoder\n",
    "        resnet_channels = encoder_channels[-1]\n",
    "        self.resnet = self.make_resnet_layers(channel=resnet_channels, num_layers=num_residual_blocks)\n",
    "        \n",
    "        self.decoder = self.make_decoder_layers(channels=encoder_channels[::-1], scale_factors=scale_factors[::-1], output_shape=output_shape)\n",
    "\n",
    "    def make_encoder_layers(self, channels = [1, 32, 128, 256], scale_factors = [(5,2), (4,2), (2,2)]):\n",
    "        # sequence of encoder layers\n",
    "        layers = []\n",
    "        for i in range(len(channels)-1):\n",
    "            layers.append(EncoderLayer(channels[i], channels[i+1], scale_factor=scale_factors[i]))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def make_decoder_layers(self, channels = [256, 128, 32, 16], scale_factors = [(2,2), (4,2), (5,2)], output_shape=(1000,20)):\n",
    "        # sequence of decoder layers\n",
    "        layers = []\n",
    "        for i in range(len(channels)-2):\n",
    "            layers.append(DecoderLayer(channels[i], channels[i+1], scale_factor=scale_factors[i]))\n",
    "        layers.append(DecoderLayer(channels[-2], channels[-1], last=True, output_shape=output_shape))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def make_resnet_layers(self, channel=256, num_layers=3):\n",
    "        # sequence of resnet layers\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            layers.append(ResidualBlock(channel, channel))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.resnet(x)\n",
    "        x = self.decoder(x)\n",
    "        x =x[:,-1,]\n",
    "        return x\n",
    "    \n",
    "    #load from pretrained weights\n",
    "    def load_pretrained(self, pretrained_path):\n",
    "        pretrained_dict = torch.load(pretrained_path, map_location=torch.device('cpu'))\n",
    "        model_dict = self.state_dict()\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "        \n",
    "        model_dict.update(pretrained_dict) \n",
    "        self.load_state_dict(model_dict)\n",
    "\n",
    "        del pretrained_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = 16    # Replace with the actual size of your input vocabulary\n",
    "output_dim = 20     # Assuming 3 for x, y, z coordinates in pose estimation\n",
    "hidden_dim = 256\n",
    "num_layers = 4\n",
    "num_heads = 8\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "kwargs = {\n",
    "        'data_path': '../dataset/FPE/S1/p3',\n",
    "        'seq_len':200,\n",
    "        'num_channels':16,\n",
    "        'stride':1,\n",
    "        'filter_data':True,\n",
    "        'fs':cfg.DATA.EMG.SAMPLING_RATE,\n",
    "        'Q':cfg.DATA.EMG.Q,\n",
    "        'low_freq':cfg.DATA.EMG.LOW_FREQ,\n",
    "        'high_freq':cfg.DATA.EMG.HIGH_FREQ,\n",
    "        'notch_freq':cfg.DATA.EMG.NOTCH_FREQ,\n",
    "        'ica': False,\n",
    "        'transform': None,\n",
    "        'target_transform': None,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_SOURCES = {\n",
    "    'emg': read_emg,\n",
    "    'leap': read_leap,\n",
    "}\n",
    "\n",
    "class EMGLeap(BaseDataset):\n",
    "    def __init__(self, kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # read the data\n",
    "        edf_files, csv_files = self.read_dirs()\n",
    "\n",
    "        if len(edf_files) == 0:\n",
    "            raise ValueError(f'No edf files found in {self.data_path}')\n",
    "        if len(csv_files) == 0:\n",
    "            raise ValueError(f'No csv files found in {self.data_path}')\n",
    "\n",
    "        threads = [None] * len(edf_files)\n",
    "        results = {\n",
    "            'data': [None] * len(edf_files),\n",
    "            'label': [None] * len(edf_files),\n",
    "            'gestures': [None] * len(edf_files)\n",
    "        }\n",
    "\n",
    "        #  read the data\n",
    "        self.data, self.label, self.gestures = [], [], []\n",
    "        for i in range(len(edf_files)):\n",
    "            print(f'Reading data from {edf_files[i]} and {csv_files[i]}')\n",
    "            thread = Thread(target=self.prepare_data, args=(edf_files[i], csv_files[i], results, i))\n",
    "            threads[i] = thread\n",
    "\n",
    "        for i in range(len(edf_files)):\n",
    "            threads[i].start()\n",
    "\n",
    "        for i in range(len(edf_files)):\n",
    "            threads[i].join()\n",
    "\n",
    "        self.data = np.concatenate(results['data'], axis=0)\n",
    "        self.label = np.concatenate(results['label'], axis=0)\n",
    "\n",
    "\n",
    "        # to tensor\n",
    "        self.data = torch.tensor(self.data, dtype=torch.float32)\n",
    "        self.label = torch.tensor(self.label, dtype=torch.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            self.data = self.transform(self.data)\n",
    "\n",
    "        if self.target_transform:\n",
    "            self.label = self.target_transform(self.label)\n",
    "\n",
    "    def read_dirs(self):\n",
    "        if isinstance(self.data_path, str):\n",
    "            self.data_path = [self.data_path]\n",
    "        all_files = []\n",
    "        for path in self.data_path:\n",
    "            if not os.path.isdir(path):\n",
    "                raise ValueError(f'{path} is not a directory')\n",
    "            else:\n",
    "                print(f'Reading data from {path}')\n",
    "                all_files += [f for f in glob.glob(os.path.join(path, '**/*'), recursive=True) if\n",
    "                              os.path.splitext(f)[1] in ['.edf', '.csv']]\n",
    "\n",
    "        # # Traverse through all the directories and read the data\n",
    "        # all_files = [f for f in glob.glob(os.path.join(self.data_path, '**/*'), recursive=True) if os.path.splitext(f)[1] in ['.edf', '.csv']]\n",
    "        # Separate .edf and .csv files\n",
    "\n",
    "        edf_files = sorted([file for file in all_files if file.endswith('.edf')])\n",
    "        csv_files = sorted([file for file in all_files if file.endswith('.csv')])\n",
    "\n",
    "        return edf_files, csv_files\n",
    "\n",
    "    def print_dataset_specs(self):\n",
    "        print(\"data shape: \", self.data.shape)\n",
    "\n",
    "    def prepare_data(self, data_path, label_path, results={}, index=0):\n",
    "        data, annotations, header = DATA_SOURCES['emg'](data_path)\n",
    "        label, _, _ = DATA_SOURCES['leap'](label_path, rotations=True, positions=False)\n",
    "\n",
    "        if index == 0:\n",
    "            # save the column names for the label\n",
    "            self.label_columns = list(label.columns)\n",
    "            self.data_columns = list(data.columns)\n",
    "\n",
    "        # set the start and end of experiment\n",
    "        start_time = max(min(data.index), min(label.index))\n",
    "        end_time = min(max(data.index), max(label.index))\n",
    "\n",
    "        # select only the data between start and end time\n",
    "        data = data.loc[start_time:end_time]\n",
    "        label = label.loc[start_time:end_time]\n",
    "\n",
    "        self.label_columns = list(label.columns)\n",
    "        # Merge the two DataFrames based on the 'time' column\n",
    "        merged_df = pd.merge_asof(data, label, on='time', direction='forward')\n",
    "        data = merged_df[data.columns].to_numpy()\n",
    "        label = merged_df[label.columns].to_numpy()\n",
    "\n",
    "        data = torch.tensor(data, dtype=torch.float32)\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "        #  convert into shape NxSxC with a sliding window using torch roll\n",
    "        data = data.unfold(0, self.seq_len, self.stride).permute(0, 2, 1)\n",
    "        label = label.unfold(0, self.seq_len, self.stride).permute(0, 2, 1)\n",
    "\n",
    "        # data, label, gestures = create_windowed_dataset(merged_df, annotations, self.seq_len, self.stride)\n",
    "        #  convert into shape NxSxC with a sliding window\n",
    "\n",
    "\n",
    "        # normalize the data\n",
    "        data = self.normalize_and_filter(data)\n",
    "\n",
    "        results['data'][index] = data\n",
    "        results['label'][index] = label\n",
    "    \n",
    "    \n",
    "    def normalize_and_filter(self, data=None):\n",
    "\n",
    "        N, C, L = data.shape\n",
    "        data_sliced = data.reshape(-1, L)\n",
    "\n",
    "        # normalize the data\n",
    "        scaler = StandardScaler()\n",
    "        data_sliced = scaler.fit_transform(data_sliced)\n",
    "\n",
    "        print(\"Filtering data...\")\n",
    "        # filter the data\n",
    "        if self.filter_data:\n",
    "            data_sliced = self._filter_data(data_sliced)\n",
    "\n",
    "        return data_sliced.reshape(N, C, L)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.label[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from ../dataset/FPE/S1/p3\n",
      "Reading data from ../dataset/FPE/S1/p3/fpe_pos3_001_S1_rep0_BT.edf and ../dataset/FPE/S1/p3/fpe_pos3_001_S1_rep0_BT.csv\n",
      "2024-01-02 11:17:25\n",
      "Filtering data...\n"
     ]
    }
   ],
   "source": [
    "dataset = EMGLeap(kwargs=kwargs)\n",
    "train_idx, test_idx = train_test_split(list(range(len(dataset))), test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_idx)\n",
    "test_dataset = torch.utils.data.Subset(dataset, test_idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rufael.marew/.conda/envs/fgr/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = Transformer(input_dim, output_dim, hidden_dim, num_layers, num_heads)\n",
    "model = model.to(device)\n",
    "# Loss and optimizer (using Mean Absolute Error for regression)\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14483988"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Batch 10/635, Loss: 24.1449\n",
      "Epoch 1/10, Batch 20/635, Loss: 23.3261\n",
      "Epoch 1/10, Batch 30/635, Loss: 23.4143\n",
      "Epoch 1/10, Batch 40/635, Loss: 24.2512\n",
      "Epoch 1/10, Batch 50/635, Loss: 25.6130\n",
      "Epoch 1/10, Batch 60/635, Loss: 25.3755\n",
      "Epoch 1/10, Batch 70/635, Loss: 23.4997\n",
      "Epoch 1/10, Batch 80/635, Loss: 22.0900\n",
      "Epoch 1/10, Batch 90/635, Loss: 24.3088\n",
      "Epoch 1/10, Batch 100/635, Loss: 23.3273\n",
      "Epoch 1/10, Batch 110/635, Loss: 23.1804\n",
      "Epoch 1/10, Batch 120/635, Loss: 24.2089\n",
      "Epoch 1/10, Batch 130/635, Loss: 22.4082\n",
      "Epoch 1/10, Batch 140/635, Loss: 25.0379\n",
      "Epoch 1/10, Batch 150/635, Loss: 25.2241\n",
      "Epoch 1/10, Batch 160/635, Loss: 23.7424\n",
      "Epoch 1/10, Batch 170/635, Loss: 20.6424\n",
      "Epoch 1/10, Batch 180/635, Loss: 22.4621\n",
      "Epoch 1/10, Batch 190/635, Loss: 24.6297\n",
      "Epoch 1/10, Batch 200/635, Loss: 20.2762\n",
      "Epoch 1/10, Batch 210/635, Loss: 22.8169\n",
      "Epoch 1/10, Batch 220/635, Loss: 22.4016\n",
      "Epoch 1/10, Batch 230/635, Loss: 23.2747\n",
      "Epoch 1/10, Batch 240/635, Loss: 23.3301\n",
      "Epoch 1/10, Batch 250/635, Loss: 24.7057\n",
      "Epoch 1/10, Batch 260/635, Loss: 24.6385\n",
      "Epoch 1/10, Batch 270/635, Loss: 22.3953\n",
      "Epoch 1/10, Batch 280/635, Loss: 22.9800\n",
      "Epoch 1/10, Batch 290/635, Loss: 24.8342\n",
      "Epoch 1/10, Batch 300/635, Loss: 25.0074\n",
      "Epoch 1/10, Batch 310/635, Loss: 22.6424\n",
      "Epoch 1/10, Batch 320/635, Loss: 22.3433\n",
      "Epoch 1/10, Batch 330/635, Loss: 22.2466\n",
      "Epoch 1/10, Batch 340/635, Loss: 24.4341\n",
      "Epoch 1/10, Batch 350/635, Loss: 22.9258\n",
      "Epoch 1/10, Batch 360/635, Loss: 25.0085\n",
      "Epoch 1/10, Batch 370/635, Loss: 22.1406\n",
      "Epoch 1/10, Batch 380/635, Loss: 22.4026\n",
      "Epoch 1/10, Batch 390/635, Loss: 23.6723\n",
      "Epoch 1/10, Batch 400/635, Loss: 23.1561\n",
      "Epoch 1/10, Batch 410/635, Loss: 26.0094\n",
      "Epoch 1/10, Batch 420/635, Loss: 24.0945\n",
      "Epoch 1/10, Batch 430/635, Loss: 23.0439\n",
      "Epoch 1/10, Batch 440/635, Loss: 25.5697\n",
      "Epoch 1/10, Batch 450/635, Loss: 21.6813\n",
      "Epoch 1/10, Batch 460/635, Loss: 22.9201\n",
      "Epoch 1/10, Batch 470/635, Loss: 24.9393\n",
      "Epoch 1/10, Batch 480/635, Loss: 23.6629\n",
      "Epoch 1/10, Batch 490/635, Loss: 25.5314\n",
      "Epoch 1/10, Batch 500/635, Loss: 25.4372\n",
      "Epoch 1/10, Batch 510/635, Loss: 22.7955\n",
      "Epoch 1/10, Batch 520/635, Loss: 23.5196\n",
      "Epoch 1/10, Batch 530/635, Loss: 24.4355\n",
      "Epoch 1/10, Batch 540/635, Loss: 24.2111\n",
      "Epoch 1/10, Batch 550/635, Loss: 21.1992\n",
      "Epoch 1/10, Batch 560/635, Loss: 24.9089\n",
      "Epoch 1/10, Batch 570/635, Loss: 26.0625\n",
      "Epoch 1/10, Batch 580/635, Loss: 22.9526\n",
      "Epoch 1/10, Batch 590/635, Loss: 23.1288\n",
      "Epoch 1/10, Batch 600/635, Loss: 23.5777\n",
      "Epoch 1/10, Batch 610/635, Loss: 23.5881\n",
      "Epoch 1/10, Batch 620/635, Loss: 24.2945\n",
      "Epoch 1/10, Batch 630/635, Loss: 23.8406\n",
      "Epoch 1/10, Average Training Loss: 23.5445\n",
      "Epoch 1/10, Validation Loss: 23.3440\n",
      "Epoch 2/10, Batch 10/635, Loss: 24.1214\n",
      "Epoch 2/10, Batch 20/635, Loss: 23.2779\n",
      "Epoch 2/10, Batch 30/635, Loss: 23.1568\n",
      "Epoch 2/10, Batch 40/635, Loss: 23.4427\n",
      "Epoch 2/10, Batch 50/635, Loss: 23.2858\n",
      "Epoch 2/10, Batch 60/635, Loss: 24.9586\n",
      "Epoch 2/10, Batch 70/635, Loss: 24.2999\n",
      "Epoch 2/10, Batch 80/635, Loss: 24.9708\n",
      "Epoch 2/10, Batch 90/635, Loss: 21.7908\n",
      "Epoch 2/10, Batch 100/635, Loss: 26.7235\n",
      "Epoch 2/10, Batch 110/635, Loss: 23.8086\n",
      "Epoch 2/10, Batch 120/635, Loss: 24.4990\n",
      "Epoch 2/10, Batch 130/635, Loss: 24.0211\n",
      "Epoch 2/10, Batch 140/635, Loss: 23.0210\n",
      "Epoch 2/10, Batch 150/635, Loss: 24.0151\n",
      "Epoch 2/10, Batch 160/635, Loss: 23.6798\n",
      "Epoch 2/10, Batch 170/635, Loss: 22.4403\n",
      "Epoch 2/10, Batch 180/635, Loss: 24.4452\n",
      "Epoch 2/10, Batch 190/635, Loss: 23.4269\n",
      "Epoch 2/10, Batch 200/635, Loss: 23.7566\n",
      "Epoch 2/10, Batch 210/635, Loss: 21.8850\n",
      "Epoch 2/10, Batch 220/635, Loss: 23.0112\n",
      "Epoch 2/10, Batch 230/635, Loss: 25.7317\n",
      "Epoch 2/10, Batch 240/635, Loss: 24.8788\n",
      "Epoch 2/10, Batch 250/635, Loss: 23.1043\n",
      "Epoch 2/10, Batch 260/635, Loss: 22.2684\n",
      "Epoch 2/10, Batch 270/635, Loss: 20.4913\n",
      "Epoch 2/10, Batch 280/635, Loss: 24.4695\n",
      "Epoch 2/10, Batch 290/635, Loss: 23.9590\n",
      "Epoch 2/10, Batch 300/635, Loss: 21.3774\n",
      "Epoch 2/10, Batch 310/635, Loss: 23.2481\n",
      "Epoch 2/10, Batch 320/635, Loss: 23.6972\n",
      "Epoch 2/10, Batch 330/635, Loss: 21.8058\n",
      "Epoch 2/10, Batch 340/635, Loss: 25.2055\n",
      "Epoch 2/10, Batch 350/635, Loss: 25.4140\n",
      "Epoch 2/10, Batch 360/635, Loss: 24.4940\n",
      "Epoch 2/10, Batch 370/635, Loss: 24.1359\n",
      "Epoch 2/10, Batch 380/635, Loss: 24.0880\n",
      "Epoch 2/10, Batch 390/635, Loss: 24.0895\n",
      "Epoch 2/10, Batch 400/635, Loss: 24.2358\n",
      "Epoch 2/10, Batch 410/635, Loss: 24.2157\n",
      "Epoch 2/10, Batch 420/635, Loss: 24.6990\n",
      "Epoch 2/10, Batch 430/635, Loss: 24.5637\n",
      "Epoch 2/10, Batch 440/635, Loss: 25.9379\n",
      "Epoch 2/10, Batch 450/635, Loss: 23.8310\n",
      "Epoch 2/10, Batch 460/635, Loss: 24.3211\n",
      "Epoch 2/10, Batch 470/635, Loss: 24.0334\n",
      "Epoch 2/10, Batch 480/635, Loss: 23.7660\n",
      "Epoch 2/10, Batch 490/635, Loss: 22.1139\n",
      "Epoch 2/10, Batch 500/635, Loss: 21.9131\n",
      "Epoch 2/10, Batch 510/635, Loss: 25.0555\n",
      "Epoch 2/10, Batch 520/635, Loss: 24.6389\n",
      "Epoch 2/10, Batch 530/635, Loss: 21.5903\n",
      "Epoch 2/10, Batch 540/635, Loss: 23.4053\n",
      "Epoch 2/10, Batch 550/635, Loss: 22.1152\n",
      "Epoch 2/10, Batch 560/635, Loss: 24.2358\n",
      "Epoch 2/10, Batch 570/635, Loss: 25.7285\n",
      "Epoch 2/10, Batch 580/635, Loss: 22.4643\n",
      "Epoch 2/10, Batch 590/635, Loss: 23.0164\n",
      "Epoch 2/10, Batch 600/635, Loss: 23.7575\n",
      "Epoch 2/10, Batch 610/635, Loss: 22.4894\n",
      "Epoch 2/10, Batch 620/635, Loss: 23.6387\n",
      "Epoch 2/10, Batch 630/635, Loss: 22.7956\n",
      "Epoch 2/10, Average Training Loss: 23.5376\n",
      "Epoch 2/10, Validation Loss: 23.3429\n",
      "Epoch 3/10, Batch 10/635, Loss: 23.8851\n",
      "Epoch 3/10, Batch 20/635, Loss: 23.6559\n",
      "Epoch 3/10, Batch 30/635, Loss: 23.0687\n",
      "Epoch 3/10, Batch 40/635, Loss: 24.9279\n",
      "Epoch 3/10, Batch 50/635, Loss: 24.6281\n",
      "Epoch 3/10, Batch 60/635, Loss: 22.3664\n",
      "Epoch 3/10, Batch 70/635, Loss: 24.0697\n",
      "Epoch 3/10, Batch 80/635, Loss: 23.1917\n",
      "Epoch 3/10, Batch 90/635, Loss: 24.0387\n",
      "Epoch 3/10, Batch 100/635, Loss: 23.9441\n",
      "Epoch 3/10, Batch 110/635, Loss: 23.9896\n",
      "Epoch 3/10, Batch 120/635, Loss: 22.3243\n",
      "Epoch 3/10, Batch 130/635, Loss: 21.9066\n",
      "Epoch 3/10, Batch 140/635, Loss: 23.3385\n",
      "Epoch 3/10, Batch 150/635, Loss: 22.0616\n",
      "Epoch 3/10, Batch 160/635, Loss: 23.4565\n",
      "Epoch 3/10, Batch 170/635, Loss: 23.2040\n",
      "Epoch 3/10, Batch 180/635, Loss: 23.2350\n",
      "Epoch 3/10, Batch 190/635, Loss: 22.7374\n",
      "Epoch 3/10, Batch 200/635, Loss: 23.1811\n",
      "Epoch 3/10, Batch 210/635, Loss: 22.5142\n",
      "Epoch 3/10, Batch 220/635, Loss: 23.4120\n",
      "Epoch 3/10, Batch 230/635, Loss: 25.0605\n",
      "Epoch 3/10, Batch 240/635, Loss: 21.6970\n",
      "Epoch 3/10, Batch 250/635, Loss: 23.0218\n",
      "Epoch 3/10, Batch 260/635, Loss: 25.1062\n",
      "Epoch 3/10, Batch 270/635, Loss: 24.4595\n",
      "Epoch 3/10, Batch 280/635, Loss: 23.3969\n",
      "Epoch 3/10, Batch 290/635, Loss: 21.8021\n",
      "Epoch 3/10, Batch 300/635, Loss: 22.4359\n",
      "Epoch 3/10, Batch 310/635, Loss: 21.5740\n",
      "Epoch 3/10, Batch 320/635, Loss: 23.5429\n",
      "Epoch 3/10, Batch 330/635, Loss: 23.2563\n",
      "Epoch 3/10, Batch 340/635, Loss: 25.3238\n",
      "Epoch 3/10, Batch 350/635, Loss: 24.0504\n",
      "Epoch 3/10, Batch 360/635, Loss: 23.2673\n",
      "Epoch 3/10, Batch 370/635, Loss: 24.1274\n",
      "Epoch 3/10, Batch 380/635, Loss: 23.2591\n",
      "Epoch 3/10, Batch 390/635, Loss: 22.6956\n",
      "Epoch 3/10, Batch 400/635, Loss: 22.9357\n",
      "Epoch 3/10, Batch 410/635, Loss: 24.5650\n",
      "Epoch 3/10, Batch 420/635, Loss: 24.5714\n",
      "Epoch 3/10, Batch 430/635, Loss: 24.3745\n",
      "Epoch 3/10, Batch 440/635, Loss: 21.8761\n",
      "Epoch 3/10, Batch 450/635, Loss: 23.7127\n",
      "Epoch 3/10, Batch 460/635, Loss: 23.2278\n",
      "Epoch 3/10, Batch 470/635, Loss: 23.9162\n",
      "Epoch 3/10, Batch 480/635, Loss: 25.1758\n",
      "Epoch 3/10, Batch 490/635, Loss: 23.2075\n",
      "Epoch 3/10, Batch 500/635, Loss: 23.7267\n",
      "Epoch 3/10, Batch 510/635, Loss: 24.4395\n",
      "Epoch 3/10, Batch 520/635, Loss: 22.6250\n",
      "Epoch 3/10, Batch 530/635, Loss: 26.1169\n",
      "Epoch 3/10, Batch 540/635, Loss: 23.2295\n",
      "Epoch 3/10, Batch 550/635, Loss: 23.8600\n",
      "Epoch 3/10, Batch 560/635, Loss: 23.9508\n",
      "Epoch 3/10, Batch 570/635, Loss: 22.9956\n",
      "Epoch 3/10, Batch 580/635, Loss: 23.7440\n",
      "Epoch 3/10, Batch 590/635, Loss: 22.3262\n",
      "Epoch 3/10, Batch 600/635, Loss: 24.0157\n",
      "Epoch 3/10, Batch 610/635, Loss: 23.4090\n",
      "Epoch 3/10, Batch 620/635, Loss: 23.2798\n",
      "Epoch 3/10, Batch 630/635, Loss: 23.8093\n",
      "Epoch 3/10, Average Training Loss: 23.5375\n",
      "Epoch 3/10, Validation Loss: 23.3417\n",
      "Epoch 4/10, Batch 10/635, Loss: 24.6004\n",
      "Epoch 4/10, Batch 20/635, Loss: 23.9736\n",
      "Epoch 4/10, Batch 30/635, Loss: 23.1005\n",
      "Epoch 4/10, Batch 40/635, Loss: 22.1584\n",
      "Epoch 4/10, Batch 50/635, Loss: 24.8662\n",
      "Epoch 4/10, Batch 60/635, Loss: 23.8236\n",
      "Epoch 4/10, Batch 70/635, Loss: 25.6457\n",
      "Epoch 4/10, Batch 80/635, Loss: 23.3640\n",
      "Epoch 4/10, Batch 90/635, Loss: 22.1955\n",
      "Epoch 4/10, Batch 100/635, Loss: 22.8268\n",
      "Epoch 4/10, Batch 110/635, Loss: 23.8416\n",
      "Epoch 4/10, Batch 120/635, Loss: 21.3572\n",
      "Epoch 4/10, Batch 130/635, Loss: 22.7242\n",
      "Epoch 4/10, Batch 140/635, Loss: 24.7155\n",
      "Epoch 4/10, Batch 150/635, Loss: 23.3397\n",
      "Epoch 4/10, Batch 160/635, Loss: 23.6566\n",
      "Epoch 4/10, Batch 170/635, Loss: 24.2867\n",
      "Epoch 4/10, Batch 180/635, Loss: 23.4226\n",
      "Epoch 4/10, Batch 190/635, Loss: 24.0033\n",
      "Epoch 4/10, Batch 200/635, Loss: 22.2962\n",
      "Epoch 4/10, Batch 210/635, Loss: 24.3149\n",
      "Epoch 4/10, Batch 220/635, Loss: 23.7934\n",
      "Epoch 4/10, Batch 230/635, Loss: 23.9226\n",
      "Epoch 4/10, Batch 240/635, Loss: 23.3522\n",
      "Epoch 4/10, Batch 250/635, Loss: 24.1892\n",
      "Epoch 4/10, Batch 260/635, Loss: 24.4201\n",
      "Epoch 4/10, Batch 270/635, Loss: 22.7161\n",
      "Epoch 4/10, Batch 280/635, Loss: 24.3190\n",
      "Epoch 4/10, Batch 290/635, Loss: 22.8275\n",
      "Epoch 4/10, Batch 300/635, Loss: 23.2888\n",
      "Epoch 4/10, Batch 310/635, Loss: 19.8679\n",
      "Epoch 4/10, Batch 320/635, Loss: 22.1164\n",
      "Epoch 4/10, Batch 330/635, Loss: 23.5454\n",
      "Epoch 4/10, Batch 340/635, Loss: 24.4462\n",
      "Epoch 4/10, Batch 350/635, Loss: 23.7379\n",
      "Epoch 4/10, Batch 360/635, Loss: 24.3374\n",
      "Epoch 4/10, Batch 370/635, Loss: 23.4844\n",
      "Epoch 4/10, Batch 380/635, Loss: 24.7258\n",
      "Epoch 4/10, Batch 390/635, Loss: 25.5801\n",
      "Epoch 4/10, Batch 400/635, Loss: 22.6283\n",
      "Epoch 4/10, Batch 410/635, Loss: 24.5397\n",
      "Epoch 4/10, Batch 420/635, Loss: 22.8777\n",
      "Epoch 4/10, Batch 430/635, Loss: 24.3723\n",
      "Epoch 4/10, Batch 440/635, Loss: 22.9463\n",
      "Epoch 4/10, Batch 450/635, Loss: 23.0827\n",
      "Epoch 4/10, Batch 460/635, Loss: 22.1439\n",
      "Epoch 4/10, Batch 470/635, Loss: 23.1692\n",
      "Epoch 4/10, Batch 480/635, Loss: 22.8952\n",
      "Epoch 4/10, Batch 490/635, Loss: 22.8712\n",
      "Epoch 4/10, Batch 500/635, Loss: 24.4137\n",
      "Epoch 4/10, Batch 510/635, Loss: 21.9049\n",
      "Epoch 4/10, Batch 520/635, Loss: 24.6783\n",
      "Epoch 4/10, Batch 530/635, Loss: 24.1974\n",
      "Epoch 4/10, Batch 540/635, Loss: 25.0468\n",
      "Epoch 4/10, Batch 550/635, Loss: 22.0751\n",
      "Epoch 4/10, Batch 560/635, Loss: 25.1235\n",
      "Epoch 4/10, Batch 570/635, Loss: 24.0769\n",
      "Epoch 4/10, Batch 580/635, Loss: 23.8453\n",
      "Epoch 4/10, Batch 590/635, Loss: 22.9735\n",
      "Epoch 4/10, Batch 600/635, Loss: 23.3447\n",
      "Epoch 4/10, Batch 610/635, Loss: 24.6568\n",
      "Epoch 4/10, Batch 620/635, Loss: 23.2150\n",
      "Epoch 4/10, Batch 630/635, Loss: 22.6276\n",
      "Epoch 4/10, Average Training Loss: 23.5398\n",
      "Epoch 4/10, Validation Loss: 23.3406\n",
      "Epoch 5/10, Batch 10/635, Loss: 23.6347\n",
      "Epoch 5/10, Batch 20/635, Loss: 24.3287\n",
      "Epoch 5/10, Batch 30/635, Loss: 24.1791\n",
      "Epoch 5/10, Batch 40/635, Loss: 23.2290\n",
      "Epoch 5/10, Batch 50/635, Loss: 23.7332\n",
      "Epoch 5/10, Batch 60/635, Loss: 23.6899\n",
      "Epoch 5/10, Batch 70/635, Loss: 23.6082\n",
      "Epoch 5/10, Batch 80/635, Loss: 24.3143\n",
      "Epoch 5/10, Batch 90/635, Loss: 25.2168\n",
      "Epoch 5/10, Batch 100/635, Loss: 23.8336\n",
      "Epoch 5/10, Batch 110/635, Loss: 22.8608\n",
      "Epoch 5/10, Batch 120/635, Loss: 24.5679\n",
      "Epoch 5/10, Batch 130/635, Loss: 24.6854\n",
      "Epoch 5/10, Batch 140/635, Loss: 24.0635\n",
      "Epoch 5/10, Batch 150/635, Loss: 22.2460\n",
      "Epoch 5/10, Batch 160/635, Loss: 24.5473\n",
      "Epoch 5/10, Batch 170/635, Loss: 22.4599\n",
      "Epoch 5/10, Batch 180/635, Loss: 23.1352\n",
      "Epoch 5/10, Batch 190/635, Loss: 22.6237\n",
      "Epoch 5/10, Batch 200/635, Loss: 24.4274\n",
      "Epoch 5/10, Batch 210/635, Loss: 23.3894\n",
      "Epoch 5/10, Batch 220/635, Loss: 23.7828\n",
      "Epoch 5/10, Batch 230/635, Loss: 22.5061\n",
      "Epoch 5/10, Batch 240/635, Loss: 22.6285\n",
      "Epoch 5/10, Batch 250/635, Loss: 23.5175\n",
      "Epoch 5/10, Batch 260/635, Loss: 22.7675\n",
      "Epoch 5/10, Batch 270/635, Loss: 24.6639\n",
      "Epoch 5/10, Batch 280/635, Loss: 22.7689\n",
      "Epoch 5/10, Batch 290/635, Loss: 22.3534\n",
      "Epoch 5/10, Batch 300/635, Loss: 22.5980\n",
      "Epoch 5/10, Batch 310/635, Loss: 22.7539\n",
      "Epoch 5/10, Batch 320/635, Loss: 21.9150\n",
      "Epoch 5/10, Batch 330/635, Loss: 24.8485\n",
      "Epoch 5/10, Batch 340/635, Loss: 22.5296\n",
      "Epoch 5/10, Batch 350/635, Loss: 24.6856\n",
      "Epoch 5/10, Batch 360/635, Loss: 24.0399\n",
      "Epoch 5/10, Batch 370/635, Loss: 24.4688\n",
      "Epoch 5/10, Batch 380/635, Loss: 23.3087\n",
      "Epoch 5/10, Batch 390/635, Loss: 22.6022\n",
      "Epoch 5/10, Batch 400/635, Loss: 21.5778\n",
      "Epoch 5/10, Batch 410/635, Loss: 23.8254\n",
      "Epoch 5/10, Batch 420/635, Loss: 24.9073\n",
      "Epoch 5/10, Batch 430/635, Loss: 25.0977\n",
      "Epoch 5/10, Batch 440/635, Loss: 22.4435\n",
      "Epoch 5/10, Batch 450/635, Loss: 21.9993\n",
      "Epoch 5/10, Batch 460/635, Loss: 24.8367\n",
      "Epoch 5/10, Batch 470/635, Loss: 25.1544\n",
      "Epoch 5/10, Batch 480/635, Loss: 23.5828\n",
      "Epoch 5/10, Batch 490/635, Loss: 24.9869\n",
      "Epoch 5/10, Batch 500/635, Loss: 26.0176\n",
      "Epoch 5/10, Batch 510/635, Loss: 23.4196\n",
      "Epoch 5/10, Batch 520/635, Loss: 23.7641\n",
      "Epoch 5/10, Batch 530/635, Loss: 24.9879\n",
      "Epoch 5/10, Batch 540/635, Loss: 22.9093\n",
      "Epoch 5/10, Batch 550/635, Loss: 22.2527\n",
      "Epoch 5/10, Batch 560/635, Loss: 24.2681\n",
      "Epoch 5/10, Batch 570/635, Loss: 22.9009\n",
      "Epoch 5/10, Batch 580/635, Loss: 22.0409\n",
      "Epoch 5/10, Batch 590/635, Loss: 23.0841\n",
      "Epoch 5/10, Batch 600/635, Loss: 22.6905\n",
      "Epoch 5/10, Batch 610/635, Loss: 23.9155\n",
      "Epoch 5/10, Batch 620/635, Loss: 22.4213\n",
      "Epoch 5/10, Batch 630/635, Loss: 21.1127\n",
      "Epoch 5/10, Average Training Loss: 23.5372\n",
      "Epoch 5/10, Validation Loss: 23.3394\n",
      "Epoch 6/10, Batch 10/635, Loss: 22.4059\n",
      "Epoch 6/10, Batch 20/635, Loss: 23.7272\n",
      "Epoch 6/10, Batch 30/635, Loss: 24.8162\n",
      "Epoch 6/10, Batch 40/635, Loss: 22.3277\n",
      "Epoch 6/10, Batch 50/635, Loss: 24.8943\n",
      "Epoch 6/10, Batch 60/635, Loss: 22.2321\n",
      "Epoch 6/10, Batch 70/635, Loss: 23.9750\n",
      "Epoch 6/10, Batch 80/635, Loss: 22.5573\n",
      "Epoch 6/10, Batch 90/635, Loss: 21.1332\n",
      "Epoch 6/10, Batch 100/635, Loss: 23.9763\n",
      "Epoch 6/10, Batch 110/635, Loss: 23.6096\n",
      "Epoch 6/10, Batch 120/635, Loss: 24.9535\n",
      "Epoch 6/10, Batch 130/635, Loss: 25.7053\n",
      "Epoch 6/10, Batch 140/635, Loss: 21.5032\n",
      "Epoch 6/10, Batch 150/635, Loss: 24.1792\n",
      "Epoch 6/10, Batch 160/635, Loss: 23.9210\n",
      "Epoch 6/10, Batch 170/635, Loss: 24.2925\n",
      "Epoch 6/10, Batch 180/635, Loss: 21.5730\n",
      "Epoch 6/10, Batch 190/635, Loss: 25.2073\n",
      "Epoch 6/10, Batch 200/635, Loss: 22.3395\n",
      "Epoch 6/10, Batch 210/635, Loss: 24.1131\n",
      "Epoch 6/10, Batch 220/635, Loss: 23.9408\n",
      "Epoch 6/10, Batch 230/635, Loss: 24.3844\n",
      "Epoch 6/10, Batch 240/635, Loss: 22.7711\n",
      "Epoch 6/10, Batch 250/635, Loss: 25.1188\n",
      "Epoch 6/10, Batch 260/635, Loss: 24.4862\n",
      "Epoch 6/10, Batch 270/635, Loss: 24.7494\n",
      "Epoch 6/10, Batch 280/635, Loss: 24.7291\n",
      "Epoch 6/10, Batch 290/635, Loss: 23.7668\n",
      "Epoch 6/10, Batch 300/635, Loss: 24.5274\n",
      "Epoch 6/10, Batch 310/635, Loss: 22.9941\n",
      "Epoch 6/10, Batch 320/635, Loss: 22.5975\n",
      "Epoch 6/10, Batch 330/635, Loss: 21.7726\n",
      "Epoch 6/10, Batch 340/635, Loss: 23.5936\n",
      "Epoch 6/10, Batch 350/635, Loss: 24.4392\n",
      "Epoch 6/10, Batch 360/635, Loss: 22.8588\n",
      "Epoch 6/10, Batch 370/635, Loss: 24.2854\n",
      "Epoch 6/10, Batch 380/635, Loss: 24.9733\n",
      "Epoch 6/10, Batch 390/635, Loss: 23.2575\n",
      "Epoch 6/10, Batch 400/635, Loss: 22.0035\n",
      "Epoch 6/10, Batch 410/635, Loss: 23.4612\n",
      "Epoch 6/10, Batch 420/635, Loss: 24.7207\n",
      "Epoch 6/10, Batch 430/635, Loss: 23.6979\n",
      "Epoch 6/10, Batch 440/635, Loss: 24.0138\n",
      "Epoch 6/10, Batch 450/635, Loss: 23.3737\n",
      "Epoch 6/10, Batch 460/635, Loss: 24.1373\n",
      "Epoch 6/10, Batch 470/635, Loss: 24.2435\n",
      "Epoch 6/10, Batch 480/635, Loss: 23.0186\n",
      "Epoch 6/10, Batch 490/635, Loss: 22.2572\n",
      "Epoch 6/10, Batch 500/635, Loss: 24.9054\n",
      "Epoch 6/10, Batch 510/635, Loss: 22.0246\n",
      "Epoch 6/10, Batch 520/635, Loss: 25.3270\n",
      "Epoch 6/10, Batch 530/635, Loss: 24.3255\n",
      "Epoch 6/10, Batch 540/635, Loss: 23.3964\n",
      "Epoch 6/10, Batch 550/635, Loss: 21.7537\n",
      "Epoch 6/10, Batch 560/635, Loss: 25.8803\n",
      "Epoch 6/10, Batch 570/635, Loss: 22.9373\n",
      "Epoch 6/10, Batch 580/635, Loss: 23.2496\n",
      "Epoch 6/10, Batch 590/635, Loss: 21.9794\n",
      "Epoch 6/10, Batch 600/635, Loss: 24.9992\n",
      "Epoch 6/10, Batch 610/635, Loss: 24.3618\n",
      "Epoch 6/10, Batch 620/635, Loss: 24.3134\n",
      "Epoch 6/10, Batch 630/635, Loss: 22.3065\n",
      "Epoch 6/10, Average Training Loss: 23.5380\n",
      "Epoch 6/10, Validation Loss: 23.3383\n",
      "Epoch 7/10, Batch 10/635, Loss: 22.7741\n",
      "Epoch 7/10, Batch 20/635, Loss: 22.6032\n",
      "Epoch 7/10, Batch 30/635, Loss: 24.8450\n",
      "Epoch 7/10, Batch 40/635, Loss: 24.8387\n",
      "Epoch 7/10, Batch 50/635, Loss: 23.1025\n",
      "Epoch 7/10, Batch 60/635, Loss: 24.6019\n",
      "Epoch 7/10, Batch 70/635, Loss: 21.8236\n",
      "Epoch 7/10, Batch 80/635, Loss: 22.7272\n",
      "Epoch 7/10, Batch 90/635, Loss: 22.9409\n",
      "Epoch 7/10, Batch 100/635, Loss: 23.0078\n",
      "Epoch 7/10, Batch 110/635, Loss: 24.0228\n",
      "Epoch 7/10, Batch 120/635, Loss: 21.7197\n",
      "Epoch 7/10, Batch 130/635, Loss: 23.8360\n",
      "Epoch 7/10, Batch 140/635, Loss: 25.2012\n",
      "Epoch 7/10, Batch 150/635, Loss: 24.3518\n",
      "Epoch 7/10, Batch 160/635, Loss: 24.3841\n",
      "Epoch 7/10, Batch 170/635, Loss: 23.9955\n",
      "Epoch 7/10, Batch 180/635, Loss: 23.8991\n",
      "Epoch 7/10, Batch 190/635, Loss: 23.6051\n",
      "Epoch 7/10, Batch 200/635, Loss: 23.8766\n",
      "Epoch 7/10, Batch 210/635, Loss: 23.8931\n",
      "Epoch 7/10, Batch 220/635, Loss: 25.7625\n",
      "Epoch 7/10, Batch 230/635, Loss: 24.7378\n",
      "Epoch 7/10, Batch 240/635, Loss: 23.0439\n",
      "Epoch 7/10, Batch 250/635, Loss: 24.2016\n",
      "Epoch 7/10, Batch 260/635, Loss: 22.2229\n",
      "Epoch 7/10, Batch 270/635, Loss: 25.5956\n",
      "Epoch 7/10, Batch 280/635, Loss: 23.4889\n",
      "Epoch 7/10, Batch 290/635, Loss: 22.7308\n",
      "Epoch 7/10, Batch 300/635, Loss: 23.5808\n",
      "Epoch 7/10, Batch 310/635, Loss: 24.0956\n",
      "Epoch 7/10, Batch 320/635, Loss: 22.8026\n",
      "Epoch 7/10, Batch 330/635, Loss: 23.9662\n",
      "Epoch 7/10, Batch 340/635, Loss: 23.9709\n",
      "Epoch 7/10, Batch 350/635, Loss: 22.3256\n",
      "Epoch 7/10, Batch 360/635, Loss: 22.6057\n",
      "Epoch 7/10, Batch 370/635, Loss: 23.2616\n",
      "Epoch 7/10, Batch 380/635, Loss: 23.6071\n",
      "Epoch 7/10, Batch 390/635, Loss: 25.1282\n",
      "Epoch 7/10, Batch 400/635, Loss: 24.9227\n",
      "Epoch 7/10, Batch 410/635, Loss: 24.5924\n",
      "Epoch 7/10, Batch 420/635, Loss: 23.7419\n",
      "Epoch 7/10, Batch 430/635, Loss: 23.4009\n",
      "Epoch 7/10, Batch 440/635, Loss: 25.2029\n",
      "Epoch 7/10, Batch 450/635, Loss: 23.9212\n",
      "Epoch 7/10, Batch 460/635, Loss: 23.5232\n",
      "Epoch 7/10, Batch 470/635, Loss: 23.5675\n",
      "Epoch 7/10, Batch 480/635, Loss: 23.4262\n",
      "Epoch 7/10, Batch 490/635, Loss: 23.4545\n",
      "Epoch 7/10, Batch 500/635, Loss: 22.4660\n",
      "Epoch 7/10, Batch 510/635, Loss: 25.0990\n",
      "Epoch 7/10, Batch 520/635, Loss: 22.1039\n",
      "Epoch 7/10, Batch 530/635, Loss: 24.3411\n",
      "Epoch 7/10, Batch 540/635, Loss: 24.5100\n",
      "Epoch 7/10, Batch 550/635, Loss: 23.0734\n",
      "Epoch 7/10, Batch 560/635, Loss: 24.1036\n",
      "Epoch 7/10, Batch 570/635, Loss: 24.3210\n",
      "Epoch 7/10, Batch 580/635, Loss: 24.0466\n",
      "Epoch 7/10, Batch 590/635, Loss: 23.0960\n",
      "Epoch 7/10, Batch 600/635, Loss: 24.4692\n",
      "Epoch 7/10, Batch 610/635, Loss: 25.8739\n",
      "Epoch 7/10, Batch 620/635, Loss: 23.9254\n",
      "Epoch 7/10, Batch 630/635, Loss: 25.9549\n",
      "Epoch 7/10, Average Training Loss: 23.5361\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch_idx, (input_seq, target_seq) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "        # Forward pass\n",
    "        output = model(input_seq, target_seq[:, :-1, :])  # Exclude the last pose from the target\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(output, target_seq[:, 1:, :])  # Exclude the first pose from the target\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Batch {batch_idx + 1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "            scheduler.step(loss)\n",
    "\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Average Training Loss: {average_loss:.4f}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq in val_loader:\n",
    "            # Forward pass\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            output = model(input_seq, target_seq[:, :-1, :])\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(output, target_seq[:, 1:, :])\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    average_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {average_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
