{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from threading import Thread\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from util.data import *\n",
    "from config import cfg\n",
    "from data import BaseDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = self.generate_encoding(d_model, max_len)\n",
    "\n",
    "    def generate_encoding(self, d_model, max_len):\n",
    "        encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        encoding = encoding.unsqueeze(0)\n",
    "        return encoding\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_length = x.size(1)\n",
    "        return self.encoding[:, :seq_length].to(x.device)\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, num_heads):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        self.pos_encoder = PositionalEncoding(hidden_dim)\n",
    "        self.encoder_layers = nn.TransformerEncoderLayer(hidden_dim, num_heads)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layers, num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x + self.pos_encoder(x)\n",
    "        x = x.permute(1, 0, 2)  # (seq_len, batch_size, hidden_dim)\n",
    "        output = self.encoder(x)\n",
    "        return output\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dim, num_layers, num_heads):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.embedding = nn.Linear(output_dim, hidden_dim)\n",
    "        self.pos_decoder = PositionalEncoding(hidden_dim)\n",
    "        self.decoder_layers = nn.TransformerDecoderLayer(hidden_dim, num_heads)\n",
    "        self.decoder = nn.TransformerDecoder(self.decoder_layers, num_layers)\n",
    "\n",
    "    def generate_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, x, encoder_output):\n",
    "        x = self.embedding(x)\n",
    "        x = x + self.pos_decoder(x)\n",
    "        x = x.permute(1, 0, 2)  # (seq_len, batch_size, hidden_dim)\n",
    "\n",
    "        # Generate causal mask\n",
    "        tgt_mask = self.generate_mask(x.size(0)).to(x.device)\n",
    "\n",
    "        output = self.decoder(x, encoder_output, tgt_mask=tgt_mask)\n",
    "        return output\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, num_layers, num_heads):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = TransformerEncoder(input_dim, hidden_dim, num_layers, num_heads)\n",
    "        self.decoder = TransformerDecoder(output_dim, hidden_dim, num_layers, num_heads)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.pos_encoder = PositionalEncoding(hidden_dim)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        encoder_output = self.encoder(x)\n",
    "        decoder_output = self.decoder(y, encoder_output)\n",
    "        output = self.fc(decoder_output)\n",
    "        # reshape back to batch_size x seq_len x num_channels\n",
    "        output = output.permute(1, 0, 2)\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "    \n",
    "# Encoder layer\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, scale_factor=(2,2)):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=(3,2), stride=(1,1), padding=(2,1)),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(3,3), stride=scale_factor),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x.float())\n",
    "        return x\n",
    "    \n",
    "class Conv3DEncoderLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, scale_factor=(2,2)):\n",
    "        super(Conv3DEncoderLayer, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, kernel_size=(3,3,2), stride=(1,1,1), padding=(1,1,1)),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(3,3,3), stride=scale_factor),\n",
    "        )\n",
    "    \n",
    "# Decoder layer\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, last=False, scale_factor=(2,2), output_shape=(1000,20)):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=(3,2), stride=(1,1)),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        if last:\n",
    "            self.decoder.append(nn.Upsample(size=output_shape, mode='bilinear', align_corners=False))\n",
    "        else:\n",
    "            self.decoder.append(nn.Upsample(scale_factor=scale_factor, mode='bilinear', align_corners=False))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "# Construct a model with 3 conv layers 3 residual blocks and 3 deconv layers using the ResNet architecture\n",
    "class NeuroPose(nn.Module):\n",
    "    def __init__(self, in_channels=1, num_residual_blocks=3, output_shape=(1000,20)):\n",
    "        super(NeuroPose, self).__init__()\n",
    "        \n",
    "        encoder_channels = [in_channels, 32, 128, 256]\n",
    "        scale_factors = [(5,2), (4,2), (2,2)]\n",
    "\n",
    "        self.encoder = self.make_encoder_layers(channels=encoder_channels, scale_factors=scale_factors)\n",
    "\n",
    "        # get last number of filters from encoder\n",
    "        resnet_channels = encoder_channels[-1]\n",
    "        self.resnet = self.make_resnet_layers(channel=resnet_channels, num_layers=num_residual_blocks)\n",
    "        \n",
    "        self.decoder = self.make_decoder_layers(channels=encoder_channels[::-1], scale_factors=scale_factors[::-1], output_shape=output_shape)\n",
    "\n",
    "    def make_encoder_layers(self, channels = [1, 32, 128, 256], scale_factors = [(5,2), (4,2), (2,2)]):\n",
    "        # sequence of encoder layers\n",
    "        layers = []\n",
    "        for i in range(len(channels)-1):\n",
    "            layers.append(EncoderLayer(channels[i], channels[i+1], scale_factor=scale_factors[i]))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def make_decoder_layers(self, channels = [256, 128, 32, 16], scale_factors = [(2,2), (4,2), (5,2)], output_shape=(1000,20)):\n",
    "        # sequence of decoder layers\n",
    "        layers = []\n",
    "        for i in range(len(channels)-2):\n",
    "            layers.append(DecoderLayer(channels[i], channels[i+1], scale_factor=scale_factors[i]))\n",
    "        layers.append(DecoderLayer(channels[-2], channels[-1], last=True, output_shape=output_shape))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def make_resnet_layers(self, channel=256, num_layers=3):\n",
    "        # sequence of resnet layers\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            layers.append(ResidualBlock(channel, channel))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.resnet(x)\n",
    "        x = self.decoder(x)\n",
    "        x =x[:,-1,]\n",
    "        return x\n",
    "    \n",
    "    #load from pretrained weights\n",
    "    def load_pretrained(self, pretrained_path):\n",
    "        pretrained_dict = torch.load(pretrained_path, map_location=torch.device('cpu'))\n",
    "        model_dict = self.state_dict()\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "        \n",
    "        model_dict.update(pretrained_dict) \n",
    "        self.load_state_dict(model_dict)\n",
    "\n",
    "        del pretrained_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = 16    # Replace with the actual size of your input vocabulary\n",
    "output_dim = 20     # Assuming 3 for x, y, z coordinates in pose estimation\n",
    "hidden_dim = 256\n",
    "num_layers = 4\n",
    "num_heads = 8\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "kwargs = {\n",
    "        'data_path': '../dataset/FPE/S1/p3',\n",
    "        'seq_len':1000,\n",
    "        'num_channels':16,\n",
    "        'stride':1,\n",
    "        'filter_data':True,\n",
    "        'fs':cfg.DATA.EMG.SAMPLING_RATE,\n",
    "        'Q':cfg.DATA.EMG.Q,\n",
    "        'low_freq':cfg.DATA.EMG.LOW_FREQ,\n",
    "        'high_freq':cfg.DATA.EMG.HIGH_FREQ,\n",
    "        'notch_freq':cfg.DATA.EMG.NOTCH_FREQ,\n",
    "        'ica': False,\n",
    "        'transform': None,\n",
    "        'target_transform': None,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_SOURCES = {\n",
    "    'emg': read_emg,\n",
    "    'leap': read_leap,\n",
    "}\n",
    "\n",
    "class EMGLeap(BaseDataset):\n",
    "    def __init__(self, kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # read the data\n",
    "        edf_files, csv_files = self.read_dirs()\n",
    "\n",
    "        if len(edf_files) == 0:\n",
    "            raise ValueError(f'No edf files found in {self.data_path}')\n",
    "        if len(csv_files) == 0:\n",
    "            raise ValueError(f'No csv files found in {self.data_path}')\n",
    "\n",
    "        threads = [None] * len(edf_files)\n",
    "        results = {\n",
    "            'data': [None] * len(edf_files),\n",
    "            'label': [None] * len(edf_files),\n",
    "            'gestures': [None] * len(edf_files)\n",
    "        }\n",
    "\n",
    "        #  read the data\n",
    "        self.data, self.label, self.gestures = [], [], []\n",
    "        for i in range(len(edf_files)):\n",
    "            print(f'Reading data from {edf_files[i]} and {csv_files[i]}')\n",
    "            thread = Thread(target=self.prepare_data, args=(edf_files[i], csv_files[i], results, i))\n",
    "            threads[i] = thread\n",
    "\n",
    "        for i in range(len(edf_files)):\n",
    "            threads[i].start()\n",
    "\n",
    "        for i in range(len(edf_files)):\n",
    "            threads[i].join()\n",
    "\n",
    "        self.data = np.concatenate(results['data'], axis=0)\n",
    "        self.label = np.concatenate(results['label'], axis=0)\n",
    "\n",
    "        #  print dataset specs\n",
    "        self.print_dataset_specs()\n",
    "\n",
    "        if self.ica:\n",
    "            self.apply_ica_to_emg()\n",
    "        else:\n",
    "            self.data_ica = None\n",
    "            self.mixing_matrix = None\n",
    "\n",
    "        # to tensor\n",
    "        self.data = torch.tensor(self.data, dtype=torch.float32)\n",
    "        self.label = torch.tensor(self.label, dtype=torch.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            self.data = self.transform(self.data)\n",
    "\n",
    "        if self.target_transform:\n",
    "            self.label = self.target_transform(self.label)\n",
    "\n",
    "    def read_dirs(self):\n",
    "        if isinstance(self.data_path, str):\n",
    "            self.data_path = [self.data_path]\n",
    "        all_files = []\n",
    "        for path in self.data_path:\n",
    "            if not os.path.isdir(path):\n",
    "                raise ValueError(f'{path} is not a directory')\n",
    "            else:\n",
    "                print(f'Reading data from {path}')\n",
    "                all_files += [f for f in glob.glob(os.path.join(path, '**/*'), recursive=True) if\n",
    "                              os.path.splitext(f)[1] in ['.edf', '.csv']]\n",
    "\n",
    "        # # Traverse through all the directories and read the data\n",
    "        # all_files = [f for f in glob.glob(os.path.join(self.data_path, '**/*'), recursive=True) if os.path.splitext(f)[1] in ['.edf', '.csv']]\n",
    "        # Separate .edf and .csv files\n",
    "\n",
    "        edf_files = sorted([file for file in all_files if file.endswith('.edf')])\n",
    "        csv_files = sorted([file for file in all_files if file.endswith('.csv')])\n",
    "\n",
    "        return edf_files, csv_files\n",
    "\n",
    "    def print_dataset_specs(self):\n",
    "        print(\"data shape: \", self.data.shape)\n",
    "\n",
    "    def prepare_data(self, data_path, label_path, results={}, index=0):\n",
    "        data, annotations, header = DATA_SOURCES['emg'](data_path)\n",
    "        label, _, _ = DATA_SOURCES['leap'](label_path, rotations=True, positions=False)\n",
    "\n",
    "        if index == 0:\n",
    "            # save the column names for the label\n",
    "            self.label_columns = list(label.columns)\n",
    "            self.data_columns = list(data.columns)\n",
    "\n",
    "        # set the start and end of experiment\n",
    "        start_time = max(min(data.index), min(label.index))\n",
    "        end_time = min(max(data.index), max(label.index))\n",
    "\n",
    "        # select only the data between start and end time\n",
    "        data = data.loc[start_time:end_time]\n",
    "        label = label.loc[start_time:end_time]\n",
    "\n",
    "        self.label_columns = list(label.columns)\n",
    "        # Merge the two DataFrames based on the 'time' column\n",
    "        merged_df = pd.merge_asof(data, label, on='time', direction='forward')\n",
    "        data = merged_df[data.columns].to_numpy()\n",
    "        label = merged_df[label.columns].to_numpy()\n",
    "\n",
    "        data = torch.tensor(data, dtype=torch.float32)\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "        #  convert into shape NxSxC with a sliding window using torch roll\n",
    "        data = data.unfold(0, self.seq_len, self.stride).permute(0, 2, 1)\n",
    "        label = label.unfold(0, self.seq_len, self.stride).permute(0, 2, 1)\n",
    "\n",
    "        # data, label, gestures = create_windowed_dataset(merged_df, annotations, self.seq_len, self.stride)\n",
    "        #  convert into shape NxSxC with a sliding window\n",
    "\n",
    "\n",
    "        # normalize the data\n",
    "        data = self.normalize_and_filter(data)\n",
    "\n",
    "        results['data'][index] = data\n",
    "        results['label'][index] = label\n",
    "    \n",
    "    \n",
    "    def normalize_and_filter(self, data=None):\n",
    "\n",
    "        N, C, L = data.shape\n",
    "        data_sliced = data.reshape(-1, L)\n",
    "\n",
    "        # normalize the data\n",
    "        scaler = StandardScaler()\n",
    "        data_sliced = scaler.fit_transform(data_sliced)\n",
    "\n",
    "        print(\"Filtering data...\")\n",
    "        # filter the data\n",
    "        if self.filter_data:\n",
    "            data_sliced = self._filter_data(data_sliced)\n",
    "\n",
    "        return data_sliced.reshape(N, C, L)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.label[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from ../dataset/FPE/S1/p3\n",
      "Reading data from ../dataset/FPE/S1/p3/fpe_pos3_001_S1_rep0_BT.edf and ../dataset/FPE/S1/p3/fpe_pos3_001_S1_rep0_BT.csv\n",
      "2024-01-02 11:17:25\n",
      "Filtering data...\n",
      "data shape:  (25374, 200, 16)\n"
     ]
    }
   ],
   "source": [
    "dataset = EMGLeap(kwargs=kwargs)\n",
    "train_idx, test_idx = train_test_split(list(range(len(dataset))), test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_idx)\n",
    "test_dataset = torch.utils.data.Subset(dataset, test_idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rufaelmarew/opt/anaconda3/envs/test/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = Transformer(input_dim, output_dim, hidden_dim, num_layers, num_heads)\n",
    "model = model.to(device)\n",
    "# Loss and optimizer (using Mean Absolute Error for regression)\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch_idx, (input_seq, target_seq) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "        # Forward pass\n",
    "        output = model(input_seq, target_seq[:, :-1, :])  # Exclude the last pose from the target\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(output, target_seq[:, 1:, :])  # Exclude the first pose from the target\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "        \n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Batch {batch_idx + 1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Average Training Loss: {average_loss:.4f}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq in val_loader:\n",
    "            # Forward pass\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            output = model(input_seq, target_seq[:, :-1, :])\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(output, target_seq[:, 1:, :])\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    average_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {average_val_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
