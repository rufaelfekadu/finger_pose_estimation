{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook to train the model\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, scale_factor=(2,2)):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=(3,2), stride=(1,1), padding=(2,1)),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(3,3), stride=scale_factor),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        print(x.shape)\n",
    "        return x\n",
    "    \n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, last=False, scale_factor=(2,2)):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=(3,2), stride=(1,1)),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        if last:\n",
    "            self.decoder.append(nn.Upsample(size=(1000, 24), mode='bilinear', align_corners=False))\n",
    "        else:\n",
    "            self.decoder.append(nn.Upsample(scale_factor=scale_factor, mode='bilinear', align_corners=False))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.decoder(x)\n",
    "        print(x.shape)\n",
    "        return x\n",
    "    \n",
    "# Construct a model with 3 conv layers 3 residual blocks and 3 deconv layers using the ResNet architecture\n",
    "class NeuroPose(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, num_residual_blocks=3):\n",
    "        super(NeuroPose, self).__init__()\n",
    "        \n",
    "        encoder_channels = [in_channels, 32, 128, 256]\n",
    "        scale_factors = [(5,2), (4,2), (2,2)]\n",
    "\n",
    "        self.encoder = self.make_encoder_layers(channels=encoder_channels, scale_factors=scale_factors)\n",
    "\n",
    "\n",
    "        self.resnet = self.make_resnet_layers(channels=[256, 256, 256])\n",
    "        self.decoder = self.make_decoder_layers(channels=encoder_channels[::-1], scale_factors=scale_factors[::-1])\n",
    "\n",
    "    def make_encoder_layers(self, channels = [1, 32, 128, 256], scale_factors = [(5,2), (4,2), (2,2)]):\n",
    "        # sequence of encoder layers\n",
    "        layers = []\n",
    "        for i in range(len(channels)-1):\n",
    "            layers.append(EncoderLayer(channels[i], channels[i+1], scale_factor=scale_factors[i]))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def make_decoder_layers(self, channels = [256, 128, 32, 16], scale_factors = [(2,2), (4,2), (5,2)]):\n",
    "        # sequence of decoder layers\n",
    "        layers = []\n",
    "        for i in range(len(channels)-2):\n",
    "            layers.append(DecoderLayer(channels[i], channels[i+1], scale_factor=scale_factors[i]))\n",
    "\n",
    "        layers.append(DecoderLayer(channels[-2], channels[-1], last=True))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def make_resnet_layers(self, channels = [256, 256, 256]):\n",
    "        # sequence of resnet layers\n",
    "        layers = []\n",
    "        for i in range(len(channels)-1):\n",
    "            layers.append(ResidualBlock(channels[i], channels[i+1]))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.resnet(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuroPose(\n",
      "  (encoder): Sequential(\n",
      "    (0): EncoderLayer(\n",
      "      (encoder): Sequential(\n",
      "        (0): Conv2d(1, 32, kernel_size=(3, 2), stride=(1, 1), padding=(2, 1))\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaxPool2d(kernel_size=(3, 3), stride=(5, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "    )\n",
      "    (1): EncoderLayer(\n",
      "      (encoder): Sequential(\n",
      "        (0): Conv2d(32, 128, kernel_size=(3, 2), stride=(1, 1), padding=(2, 1))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaxPool2d(kernel_size=(3, 3), stride=(4, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "    )\n",
      "    (2): EncoderLayer(\n",
      "      (encoder): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(3, 2), stride=(1, 1), padding=(2, 1))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (resnet): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): DecoderLayer(\n",
      "      (decoder): Sequential(\n",
      "        (0): ConvTranspose2d(256, 128, kernel_size=(3, 2), stride=(1, 1))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Upsample(scale_factor=(2.0, 2.0), mode='bilinear')\n",
      "      )\n",
      "    )\n",
      "    (1): DecoderLayer(\n",
      "      (decoder): Sequential(\n",
      "        (0): ConvTranspose2d(128, 32, kernel_size=(3, 2), stride=(1, 1))\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Upsample(scale_factor=(4.0, 2.0), mode='bilinear')\n",
      "      )\n",
      "    )\n",
      "    (2): DecoderLayer(\n",
      "      (decoder): Sequential(\n",
      "        (0): ConvTranspose2d(32, 1, kernel_size=(3, 2), stride=(1, 1))\n",
      "        (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Upsample(size=(1000, 24), mode='bilinear')\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1000, 16])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test model\n",
    "model = NeuroPose()\n",
    "\n",
    "#dummy data\n",
    "x = torch.randn(1, 1,1000,16)\n",
    "print(model)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 200, 8])\n",
      "torch.Size([1, 128, 50, 4])\n",
      "torch.Size([1, 256, 25, 2])\n",
      "torch.Size([1, 128, 54, 6])\n",
      "torch.Size([1, 32, 224, 14])\n",
      "torch.Size([1, 1, 1000, 24])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1000, 24])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = model(x)\n",
    "y.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tau",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
